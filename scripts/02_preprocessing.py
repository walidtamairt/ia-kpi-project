"""
PrÃ©traitement â€” Nettoyage, normalisation, feature engineering, stockage SQL
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import sqlite3
import joblib
import os

print("ğŸ“‚ Chargement des donnÃ©es brutes...")
df = pd.read_csv("data/raw/tickets_raw.csv")
print(f"   {len(df)} tickets â€” {df.shape[1]} colonnes")

# â”€â”€ 1. Nettoyage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ§¹ Nettoyage...")
missing = df.isnull().sum()
if missing.any():
    df["nb_relances"].fillna(0, inplace=True)
    df["satisfaction"].fillna(df["satisfaction"].median(), inplace=True)
    print(f"   Valeurs manquantes corrigÃ©es.")
else:
    print("   Aucune valeur manquante âœ“")

dupes = df.duplicated(subset=["ticket_id"]).sum()
df.drop_duplicates(subset=["ticket_id"], inplace=True)
print(f"   Doublons supprimÃ©s : {dupes}")

df["created_at"]  = pd.to_datetime(df["created_at"])
df["resolved_at"] = pd.to_datetime(df["resolved_at"])

# â”€â”€ 2. Feature Engineering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nâš™ï¸  Feature engineering...")

df["mois"]             = df["created_at"].dt.month
df["trimestre"]        = df["created_at"].dt.quarter
df["est_weekend"]      = (df["jour_semaine"] >= 5).astype(int)
df["est_heure_creuse"] = ((df["heure_creation"] < 8) | (df["heure_creation"] > 18)).astype(int)

priority_order = {"Basse": 1, "Normale": 2, "Haute": 3, "Critique": 4}
df["priorite_num"] = df["priorite"].map(priority_order)

critical_sys = ["ERP SAP", "Serveur Web", "Base de donnÃ©es", "Active Directory"]
df["systeme_critique"]       = df["systeme"].isin(critical_sys).astype(int)
df["risk_priority_ratio"]    = df["risk_score"] / df["priorite_num"]
df["log_resolution_heures"]  = np.log1p(df["resolution_heures"])

print("   Features crÃ©Ã©es : mois, trimestre, est_weekend, est_heure_creuse,")
print("   priorite_num, systeme_critique, risk_priority_ratio, log_resolution_heures")

# â”€â”€ 3. Label encoding des cibles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
le_cat  = LabelEncoder()
le_risk = LabelEncoder()
df["categorie_label"] = le_cat.fit_transform(df["categorie"])
df["risk_label"]      = le_risk.fit_transform(df["risk_level"])

os.makedirs("models", exist_ok=True)
joblib.dump(le_cat,  "models/label_encoder_category.pkl")
joblib.dump(le_risk, "models/label_encoder_risk.pkl")
print("\n   Encodeurs sauvegardÃ©s dans models/")

# â”€â”€ 4. Sauvegarde CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs("data/processed", exist_ok=True)
df.to_csv("data/processed/tickets_processed.csv", index=False)
print(f"\nâœ… CSV sauvegardÃ© â†’ data/processed/tickets_processed.csv  {df.shape}")

# â”€â”€ 5. Stockage SQLite â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ—„ï¸  Stockage SQLite...")
conn = sqlite3.connect("data/tickets.db")
df.to_sql("tickets", conn, if_exists="replace", index=False)
conn.execute("CREATE INDEX IF NOT EXISTS idx_categorie  ON tickets(categorie)")
conn.execute("CREATE INDEX IF NOT EXISTS idx_risk_level ON tickets(risk_level)")
conn.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON tickets(created_at)")
conn.commit()

print("\nğŸ“Š Exemple requÃªte SQL â€” risque moyen par dÃ©partement :")
query = """
    SELECT departement,
           COUNT(*)                      AS nb_tickets,
           ROUND(AVG(resolution_heures), 1) AS delai_moyen_h,
           ROUND(AVG(risk_score), 1)        AS risk_moyen
    FROM   tickets
    WHERE  risk_level = 'Critique'
    GROUP  BY departement
    ORDER  BY nb_tickets DESC
"""
print(pd.read_sql_query(query, conn).to_string(index=False))
conn.close()

print("\nâœ… PrÃ©traitement terminÃ© !")